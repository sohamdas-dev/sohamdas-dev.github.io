<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Soham Das </title> <meta name="author" content="Soham Das"> <meta name="description" content="publications by categories in reversed chronological order. generated by jekyll-scholar."> <meta name="keywords" content="game-theory, optimization, reinforcement-learning, network-science"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link rel="preload" as="image" href="/assets/img/profile-picture.jpg"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sohamdas-dev.github.io/publications/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Soham</span> Das </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">publications by categories in reversed chronological order. generated by jekyll-scholar.</p> </header> <article> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ArXiv</abbr> </div> <div id="das2025lagrangian" class="col-sm-8"> <div class="title">The Lagrangian Method for Solving Constrained Markov Games</div> <div class="author"> <em>Soham Das</em>, Santiago Paternain, Luiz F. O. Chamon, and Ceyhun Eksin </div> <div class="periodical"> <em>arXiv</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2503.10561" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>We propose the concept of a Lagrangian game to solve constrained Markov games. Such games model scenarios where agents face cost constraints in addition to their individual rewards, that depend on both agent joint actions and the evolving environment state over time. Constrained Markov games form the formal mechanism behind safe multiagent reinforcement learning, providing a structured model for dynamic multiagent interactions in a multitude of settings, such as autonomous teams operating under local energy and time constraints, for example. We develop a primal-dual approach in which agents solve a Lagrangian game associated with the current Lagrange multiplier, simulate cost and reward trajectories over a fixed horizon, and update the multiplier using accrued experience. This update rule generates a new Lagrangian game, initiating the next iteration. Our key result consists in showing that the sequence of solutions to these Lagrangian games yields a nonstationary Nash solution for the original constrained Markov game.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">das2025lagrangian</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The Lagrangian Method for Solving Constrained Markov Games}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Das, Soham and Paternain, Santiago and Chamon, Luiz F. O. and Eksin, Ceyhun}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS CoML</abbr> </div> <div id="das2025localconstraints" class="col-sm-8"> <div class="title">The Lagrangian Method for Solving Locally Constrained Markov Games</div> <div class="author"> <em>Soham Das</em>, Santiago Paternain, Luiz F. O. Chamon, and Ceyhun Eksin </div> <div class="periodical"> 2025 </div> <div class="periodical"> NeurIPS 2025 Workshop on Constrained Optimization for Machine Learning (CoML), Non-archival workshop paper (Upcoming) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>We introduce a dual gradient descent method to solve Markov games subject to local but coupled constraints. These constrained Markov games (CMGs) model multiagent systems operating in dynamic environments where each agent seeks to maximize its individual utility while respecting specific local constraints. These constraints, which capture real-world limitations such as resource budgets or safety requirements, are local in the sense that they pertain to individual agents but are coupled through dependencies on other agents’ actions and the shared environment state. Our approach builds on the concept of the unconstrained Lagrangian game, which agents repeatedly solve based on the current values of their own local Lagrange multipliers. Agents simulate trajectories to estimate cumulative rewards and constraint violations, then adjust the multipliers using a dual gradient step grounded in their local observations. This primal-dual process produces a sequence of policy profiles that converge to a nonstationary and feasible Nash equilibrium of the original constrained game. This work extends previous frameworks for solving Markov games with global constraints, providing a principled approach to safe multiagent learning under localized, interdependent feasibility conditions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">das2025localconstraints</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The Lagrangian Method for Solving Locally Constrained Markov Games}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Das, Soham and Paternain, Santiago and Chamon, Luiz F. O. and Eksin, Ceyhun}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{NeurIPS 2025 Workshop on Constrained Optimization for Machine Learning (CoML), Non-archival workshop paper (Upcoming)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE CDC</abbr> </div> <div id="das2025cooperative" class="col-sm-8"> <div class="title">A Lagrangian Framework for Safe Cooperative Reinforcement Learning</div> <div class="author"> <em>Soham Das</em>, Luiz F. O. Chamon, Santiago Paternain, and Ceyhun Eksin </div> <div class="periodical"> <em>IEEE Conference on Decision and Control (Upcoming)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>We consider the problem of safe cooperative multiagent reinforcement learning (MARL) within the framework of a constrained multiagent Markov decision process (MDP). Agents share a common value function and learn to coordinate their actions to maximize a joint objective while adhering to system-level constraints. These constraints can enforce safety, reliability, or additional regulatory requirements governing the evolution of the multiagent system. We propose a Lagrangian-based approach, where agents iteratively solve a relaxed Lagrangian MDP using a joint learning mechanism. During execution, agents independently follow their policies, accumulating constraint violations over an epoch, which are then used to update the Lagrange multipliers. We show that continuous execution of this primal-dual algorithm produces episodes which are feasible almost surely. Further, we prove that the sequence of policies generated by the algorithm yields a nonstationary approximately optimal solution for the safe cooperative MARL problem.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">das2025cooperative</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Lagrangian Framework for Safe Cooperative Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Das, Soham and Chamon, Luiz F. O. and Paternain, Santiago and Eksin, Ceyhun}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Conference on Decision and Control (Upcoming)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">SIAM</abbr> </div> <div id="doi:10.1137/22M1506614" class="col-sm-8"> <div class="title">Average Submodularity of Maximizing Anticoordination in Network Games</div> <div class="author"> <em>Soham Das</em>, and Ceyhun Eksin </div> <div class="periodical"> <em>SIAM Journal on Control and Optimization</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://epubs.siam.org/doi/abs/10.1137/22M1506614" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p> Abstract. We consider the control of decentralized learning dynamics for agents in an anticoordination network game. In the anticoordination network game, there is a preferred action in the absence of neighbors’ actions, and the utility an agent receives from the preferred action decreases as more of its neighbors select the preferred action, potentially causing the agent to select a less desirable action. The decentralized dynamics that are based on the synchronous best-response dynamics converge for the considered payoffs. Given a convergent action profile, we measure anticoordination by the number of edges in the underlying graph that have at least one agent in either end of the edge not taking the preferred action. A designer wants to find an optimal set of agents to control under a finite budget in order to achieve maximum anticoordination (MAC) on game convergence as a result of the dynamics. We show that the MAC is submodular in expectation over all realizations of the payoff interaction constants in bipartite networks. The proof relies on characterizing well-behavedness of MAC instances for bipartite networks, and designing a coupling between the dynamics and another distribution preserving selection protocol, for which we can show the diminishing returns property. Utilizing this result, we obtain a performance guarantee for the greedy optimization of MAC. Finally, we provide a computational study to show the effectiveness of greedy node selection strategies to solve MAC on general bipartite networks. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">doi:10.1137/22M1506614</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Das, Soham and Eksin, Ceyhun}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Average Submodularity of Maximizing Anticoordination in Network Games}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{SIAM Journal on Control and Optimization}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{62}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2639-2663}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1137/22M1506614}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{https://epubs.siam.org/doi/pdf/10.1137/22M1506614}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE LCSS</abbr> </div> <div id="10531766" class="col-sm-8"> <div class="title">Learning Nash in Constrained Markov Games With an α -Potential</div> <div class="author"> <em>Soham Das</em>, and Ceyhun Eksin </div> <div class="periodical"> <em>IEEE Control Systems Letters</em>. Winner, <em>Outstanding Student Paper Prize</em>, IEEE Control Systems Society, TC Networked Systems , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/10531766" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>We develop a best-response algorithm for solving constrained Markov games assuming limited violations for the potential game property. The limited violations of the potential game property mean that changes in value function due to unilateral policy alterations can be measured by the potential function up to an error α . We show the existence of stationary ϵ -approximate constrained Nash policy whenever the set of feasible stationary policies is non-empty. Our setting has agents accessing an efficient probably approximately correct solver for a constrained Markov decision process which they use for generating best-response policies against the other agents’ former policies. For an accuracy threshold ϵ&gt;4α , the best-response dynamics generate provable convergence to ϵ -Nash policy in finite time with probability at least 1−δ at the expense of polynomial bounds on sample complexity that scales with the reciprocal of ϵ and δ .</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10531766</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Das, Soham and Eksin, Ceyhun}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Control Systems Letters}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning Nash in Constrained Markov Games With an α -Potential}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{808-813}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Games;Stochastic processes;Picture archiving and communication systems;Kernel;Finite element analysis;Complexity theory;Vehicle dynamics;Game theory;constrained control;optimization;machine learning}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/LCSYS.2024.3402132}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IISE Tran.</abbr> </div> <div id="karthikeyan2023statistical" class="col-sm-8"> <div class="title">Statistical and dynamic model of surface morphology evolution during polishing in additive manufacturing</div> <div class="author"> Adithyaa Karthikeyan, <em>Soham Das</em>, Satish TS Bukkapatnam, and Ceyhun Eksin </div> <div class="periodical"> <em>IISE Transactions</em>. Featured in <em>ISE Magazine</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.tandfonline.com/doi/abs/10.1080/24725854.2023.2264889" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Many industrial components, especially those realized through 3D printing undergo surface finishing processes, predominantly, in the form of mechanical polishing. The polishing processes for custom components remain manual and iterative. Determination of the polishing endpoints, i.e., when to stop the process to achieve a desired surface finish, remains a major obstacle to process automation and in the cost-effective custom/3D printing process chains. With the motivation to automate the polishing process of 3D printed materials to a desired level of surface smoothness, we propose a dynamic model of surface morphology evolution of 3D printed materials during a polishing process. The dynamic model can account for both material removal and redistribution during the polishing process. In addition, the model accounts for increased material flow due to heat generated during the polishing process. We also provide an initial random surface model that matches the initial surface statistics. We propose an optimization problem for model parameter estimation based on empirical data using KL-divergence and surface roughness as two metrics of the objective. We validate the proposed model using data from polishing of a 3D printed sample. The procedures developed makes the model applicable to other 3D printed materials and polishing processes. We obtain a network formation model as a representation of the surface evolution from the heights and radii of asperities. We use the network connectivity (Fiedler number) as a metric for surface smoothness that can be used to determine whether a desired level of smoothness is reached or not.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">karthikeyan2023statistical</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Statistical and dynamic model of surface morphology evolution during polishing in additive manufacturing}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Karthikeyan, Adithyaa and Das, Soham and Bukkapatnam, Satish TS and Eksin, Ceyhun}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IISE Transactions}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--15}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Taylor \&amp; Francis}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE CDC</abbr> </div> <div id="9993180" class="col-sm-8"> <div class="title">Approximate Submodularity of Maximizing Anticoordination in Network Games</div> <div class="author"> <em>Soham Das</em>, and Ceyhun Eksin </div> <div class="periodical"> <em>In 2022 IEEE 61st Conference on Decision and Control (CDC)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/9993180" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>In this paper we show that MAC is approximately submodular, hence a greedy approach to selecting control agents guarantees fractionally sub-optimal solutions for the line network. Moreover, we provide a special scenario for bipartite graphs that shows that the violation of submodularity can be arbitrarily bad, highlighting limitations of our provided guarantees in such worst case situations, thereby indicating that greedy approaches may not be always desirable in attaining anti-coordination. We further provide encouraging computational results which strongly suggest the effectiveness of greedy selection in practical scenarios.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">9993180</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Das, Soham and Eksin, Ceyhun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2022 IEEE 61st Conference on Decision and Control (CDC)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Approximate Submodularity of Maximizing Anticoordination in Network Games}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3151-3157}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Heuristic algorithms;Sociology;Games;Approximation algorithms;Cognition;Statistics;Convergence}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/CDC51059.2022.9993180}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Soham Das. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>