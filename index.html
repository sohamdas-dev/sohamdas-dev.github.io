<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Soham Das </title> <meta name="author" content="Soham Das"> <meta name="description" content="Research website. "> <meta name="keywords" content="game-theory, optimization, reinforcement-learning, network-science"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link rel="preload" as="image" href="/assets/img/profile-picture.jpg"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sohamdas-dev.github.io/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Soham</span> Das </h1> <p class="desc"><span style="font-weight: bold;">Assistant Professor, The University of Tennessee</span><br> <span style="color: #007bff;">Multiagent Systems and Complexity group</span> </p> </header> <article> <div class="profile float-right"> <figure> <picture> <img src="/assets/img/prof_pic.jpg" class="img-fluid z-depth-1 rounded" width="200" height="200" alt="prof_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="more-info"> <p>520 John D. Tickle Engineering Building,</p> <p>817 Neyland Drive,</p> <p>Knoxville, TN 37996</p> </div> </div> <div class="clearfix"> <div style="text-align: justify;"> <p>Welcome! I am an Assistant Professor in the <a href="https://ise.utk.edu/" rel="external nofollow noopener" target="_blank">Department of Industrial and Systems Engineering</a> at the University of Tennessee, Knoxville. My appointment is part of the <a href="https://research.utk.edu/cluster-hire/science-informed-artificial-intelligence/" rel="external nofollow noopener" target="_blank">Science-Informed Artificial Intelligence cluster</a> at Tennessee, a strategic initiative at the intersection of AI and the domain sciences. I also hold a courtesy appointment in the <a href="https://math.utk.edu/" rel="external nofollow noopener" target="_blank">Department of Mathematics</a>. I received my Ph.D. in Operations Research from Texas A&amp;M University in 2025, where I was advised by <a href="https://netmas.engr.tamu.edu/people/ceyhun-eksin/" rel="external nofollow noopener" target="_blank">Ceyhun Eksin</a>.</p> <p>My research sits at the interface of game theory, optimization, and reinforcement learning, with a focus on multiagent decision making in complex environments. I address challenges in designing safe and efficient learning algorithms for agents operating in dynamic systems, such as energy grids, autonomous transportation networks, and social or economic systems, where strategic interactions, uncertainity and constraints are prevalent. Topics of interest include intervention design in network games, decentralized learning in Markov games, combinatorial optimization, with applications in network analysis, epidemics and multiagent reinforcement learning.</p> <p>You can find my academic <a href="https://drive.google.com/file/d/1_HCsmwvltMVZ-OGhEd8D21tD04m3DctX/view?usp=sharing" rel="external nofollow noopener" target="_blank">CV here</a>. For an updated list of my publications, please see my <a href="https://scholar.google.com/citations?user=EeyWLicAAAAJ&amp;hl=en&amp;oi=ao" rel="external nofollow noopener" target="_blank">Google Scholar profile</a>. I am also on <a href="https://www.linkedin.com/in/soham-das-196075125/" rel="external nofollow noopener" target="_blank">LinkedIn</a>, and can be reached at sdas43 [at] utk [dot] edu.</p> </div> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Jul 22, 2025</th> <td> Honored to share that our paper, <a href="https://ieeexplore.ieee.org/abstract/document/10531766" rel="external nofollow noopener" target="_blank"><em>Learning Nash in Constrained Markov Games with an Alpha-Potential</em></a>, won the <strong>Outstanding Student Paper Prize</strong> from the <a href="https://ieeecss.org/technical-committee/networks-and-communications" rel="external nofollow noopener" target="_blank">Technical Committee on Networked Systems</a>, IEEE Control Systems Society! Many thanks to my collaborators, mentors, and the research community for their support. </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 16, 2025</th> <td> New acceptance alert: <em> A Lagrangian Framework for Safe Cooperative Reinforcement Learning </em> accepted for presentation at IEEE CDC 2025! </td> </tr> <tr> <th scope="row" style="width: 20%">May 01, 2025</th> <td> Presented my poster on <a href="https://drive.google.com/file/d/18-IhNqaGfq9nxIhdxYxlveywRXtWTzff/view?usp=share_link" rel="external nofollow noopener" target="_blank">Foundations for safe-MARL</a> at the <strong>Texas Colloquium on Distributed Learning</strong>, hosted by Rice University. </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 31, 2025</th> <td> New submission alert: <em> A Lagrangian Framework for Safe Cooperative Reinforcement Learning </em> submitted to IEEE CDC 2025! </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 13, 2025</th> <td> New preprint alert: <em> The Lagrangian Method for Solving Constrained Markov Games </em> is now available on <a href="https://arxiv.org/abs/2503.10561" rel="external nofollow noopener" target="_blank">arXiv</a>! </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 12, 2025</th> <td> Delivered a talk on <em>Safe Learning and Alignment in Multiagent Systems</em> at the <a href="https://oden.utexas.edu/news-and-events/events/2061---Soham%20Das/" rel="external nofollow noopener" target="_blank">Autonomy Seminar</a> in the Oden Institute for Computational Engineering and Sciences, the University of Texas at Austin. </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 29, 2024</th> <td> Gave a talk on <em>Asynchronous Best-Response for Learning Nash in Constrained Markov Games with an almost potential</em> at <strong>ASILOMAR 2024</strong> in the invited session “Multiagent Reinforcement Learning”, organized by Prof. Santiago Paternain. </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 22, 2024</th> <td> Gave a talk on <em>State-Augmented Reinforcement Learning for Constrained Markov Games</em> at <strong>INFORMS 2024</strong> in the contributed session “Advances in Reinforcement Learning”. </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 21, 2024</th> <td> Gave a talk on <em>Learning Nash in Constrained Markov Games with Best-Response</em> at <strong>INFORMS 2024</strong> in the invited session “Equilibrium in Complicated Games and Applications”, organized by Prof. Jeff Shamma and Prof. Lichun Li. </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 14, 2024</th> <td> Organized an invited session with Prof. Ceyhun Eksin on <em> MARL in Dynamic Games </em> at <strong>MOPTA 2024.</strong> </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">SICON</abbr> </div> <div id="doi:10.1137/22M1506614" class="col-sm-8"> <div class="title">Average Submodularity of Maximizing Anticoordination in Network Games</div> <div class="author"> <em>Soham Das</em>, and Ceyhun Eksin </div> <div class="periodical"> <em>SIAM Journal on Control and Optimization</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://epubs.siam.org/doi/abs/10.1137/22M1506614" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p> Abstract. We consider the control of decentralized learning dynamics for agents in an anticoordination network game. In the anticoordination network game, there is a preferred action in the absence of neighbors’ actions, and the utility an agent receives from the preferred action decreases as more of its neighbors select the preferred action, potentially causing the agent to select a less desirable action. The decentralized dynamics that are based on the synchronous best-response dynamics converge for the considered payoffs. Given a convergent action profile, we measure anticoordination by the number of edges in the underlying graph that have at least one agent in either end of the edge not taking the preferred action. A designer wants to find an optimal set of agents to control under a finite budget in order to achieve maximum anticoordination (MAC) on game convergence as a result of the dynamics. We show that the MAC is submodular in expectation over all realizations of the payoff interaction constants in bipartite networks. The proof relies on characterizing well-behavedness of MAC instances for bipartite networks, and designing a coupling between the dynamics and another distribution preserving selection protocol, for which we can show the diminishing returns property. Utilizing this result, we obtain a performance guarantee for the greedy optimization of MAC. Finally, we provide a computational study to show the effectiveness of greedy node selection strategies to solve MAC on general bipartite networks. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">doi:10.1137/22M1506614</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Das, Soham and Eksin, Ceyhun}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Average Submodularity of Maximizing Anticoordination in Network Games}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{SIAM Journal on Control and Optimization}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{62}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2639-2663}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1137/22M1506614}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{https://epubs.siam.org/doi/pdf/10.1137/22M1506614}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Preprint</abbr> </div> <div id="das2025lagrangian" class="col-sm-8"> <div class="title">The Lagrangian Method for Solving Constrained Markov Games</div> <div class="author"> <em>Soham Das</em>, Santiago Paternain, Luiz F. O. Chamon, and Ceyhun Eksin </div> <div class="periodical"> <em>arXiv</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2503.10561" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>We propose the concept of a Lagrangian game to solve constrained Markov games. Such games model scenarios where agents face cost constraints in addition to their individual rewards, that depend on both agent joint actions and the evolving environment state over time. Constrained Markov games form the formal mechanism behind safe multiagent reinforcement learning, providing a structured model for dynamic multiagent interactions in a multitude of settings, such as autonomous teams operating under local energy and time constraints, for example. We develop a primal-dual approach in which agents solve a Lagrangian game associated with the current Lagrange multiplier, simulate cost and reward trajectories over a fixed horizon, and update the multiplier using accrued experience. This update rule generates a new Lagrangian game, initiating the next iteration. Our key result consists in showing that the sequence of solutions to these Lagrangian games yields a nonstationary Nash solution for the original constrained Markov game.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">das2025lagrangian</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The Lagrangian Method for Solving Constrained Markov Games}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Das, Soham and Paternain, Santiago and Chamon, Luiz F. O. and Eksin, Ceyhun}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE CDC</abbr> </div> <div id="das2025cooperative" class="col-sm-8"> <div class="title">A Lagrangian Framework for Safe Cooperative Reinforcement Learning</div> <div class="author"> <em>Soham Das</em>, Luiz F. O. Chamon, Santiago Paternain, and Ceyhun Eksin </div> <div class="periodical"> <em>IEEE Conference on Decision and Control (Upcoming)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/html/" class="btn btn-sm z-depth-0" role="button">HTML</a> </div> <div class="abstract hidden"> <p>We consider the problem of safe cooperative multiagent reinforcement learning (MARL) within the framework of a constrained multiagent Markov decision process (MDP). Agents share a common value function and learn to coordinate their actions to maximize a joint objective while adhering to system-level constraints. These constraints can enforce safety, reliability, or additional regulatory requirements governing the evolution of the multiagent system. We propose a Lagrangian-based approach, where agents iteratively solve a relaxed Lagrangian MDP using a joint learning mechanism. During execution, agents independently follow their policies, accumulating constraint violations over an epoch, which are then used to update the Lagrange multipliers. We show that continuous execution of this primal-dual algorithm produces episodes which are feasible almost surely. Further, we prove that the sequence of policies generated by the algorithm yields a nonstationary approximately optimal solution for the safe cooperative MARL problem.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">das2025cooperative</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Lagrangian Framework for Safe Cooperative Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Das, Soham and Chamon, Luiz F. O. and Paternain, Santiago and Eksin, Ceyhun}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Conference on Decision and Control (Upcoming)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%73%64%61%73%34%33@%75%74%6B.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=EeyWLicAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.linkedin.com/in/soham-das-196075125" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> </div> <div class="contact-note">Always looking to collaborate. If you are broadly interested in optimization and learning in multi-agent systems, feel free to reach out. </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Soham Das. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>